# üñºÔ∏è Image Caption Generator (LLaVA)

A simple web application that generates image captions using a powerful vision-language model via FastAPI backend and a Streamlit frontend. It runs locally and accepts image uploads to return meaningful captions generated by LLaVA.

## üöÄ Features

- Upload `.png`, `.jpg`, or `.jpeg` images
- Generate AI-powered captions describing the uploaded image
- FastAPI backend for processing
- Streamlit frontend with a clean and minimal UI
- Fully local execution (no external API calls)

## üõ† Tech Stack

**Frontend**: Streamlit | **Backend**: FastAPI | **LLM Runtime**: Ollama | **Language Model**: LLaVA | **HTTP Requests**: Python `requests` library


---
## ‚öôÔ∏è Setup Instructions

1.  **Clone the repository**:
    ```bash
    git clone [https://github.com/your-username/llama-text-summarizer.git](https://github.com/your-username/llama-text-summarizer.git)
    cd llama-text-summarizer
    ```

2.  **Create and activate a Python virtual environment**:
    ```bash
    python -m venv venv
    # On macOS/Linux:
    source venv/bin/activate
    # On Windows (PowerShell):
    .\venv\Scripts\Activate.ps1
    # Or Windows (CMD):
    .\venv\Scripts/activate.bat
    ```

3.  **Install required Python packages**:
    ```bash
    pip install -r requirements.txt
    ```
    If you don't have a `requirements.txt` file, install dependencies manually:
    ```bash
    pip install fastapi uvicorn streamlit requests python-multipart
    ```

4.  **Install Ollama and download LLaMA 2 model**:
    Download and install Ollama from [https://ollama.com](https://ollama.com)
    Then, pull the LLaMA 2 model locally:
    ```bash
    ollama pull llama2
    ```

---

## ‚ñ∂Ô∏è Running the Application

1.  **Start the FastAPI backend server**:
    ```bash
    uvicorn backend.main:app --reload
    ```

2.  **In a new terminal, start the Streamlit frontend**:
    ```bash
    streamlit run frontend/app.py
    ```
    Open your browser to `http://localhost:8501`.

---

## üìù Usage

Simply enter or paste any long text into the input box on the Streamlit page and click "Summarize". The backend will send your text to LLaMA 2 running in Ollama and display the generated summary below the input box.

---
